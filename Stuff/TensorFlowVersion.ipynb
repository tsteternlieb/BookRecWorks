{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d9471be",
   "metadata": {},
   "source": [
    "## File for training book vector embeddings\n",
    "###  Contrastive Learning -> No SoftMax -> Yay!, \n",
    "### this would be fine for 10000 books, but scalability is always imporant\n",
    " - There was some sort of lit2vec repo linked in the download, but I wanted to build my own to make sure it worked with the query embed\n",
    "\n",
    "### Main Idea:\n",
    "- Generate \"positive\" pairs which are books that the same user like, or books with the same tag as well as negative pairs to prevent mode collapse. Feed each pair into the model and have the model learn which are positive and which are negative. The model will learn usefull representations of our books\n",
    "\n",
    "- This is essentially word2vec where contexts our generated a bit differently.\n",
    "\n",
    "- Lots take from the tensorflow page on Word2Vec\n",
    "\n",
    "### Improvements that should be made:\n",
    "Better Target Embedding:\n",
    "- Take into account more than a single book. I think this would be done well by a transformer. The transformer part could also then be used to find an embedding of a user based on what books they had interacted with which would be good for perseonalization.   \n",
    "\n",
    "Better Sampling: \n",
    "- Sampling is uniform as opposed to some distribution paramaterized by occurence frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b28eb524",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import random\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import collections\n",
    "import string\n",
    "import io\n",
    "import json\n",
    "import codecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da4b0a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dict(df,keys,values):\n",
    "    _list = list(zip(df[keys],df[values]))\n",
    "    c = collections.defaultdict(list)\n",
    "    for a,b in _list:\n",
    "        c[a].extend([b])\n",
    "\n",
    "    for key in list(c.keys()):\n",
    "        if len(c[key]) < 2:\n",
    "            del c[key]\n",
    "    print(len(c),'c length')\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e9178b",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "836d9415",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b = pd.read_csv( 'books.csv' )\n",
    "t = pd.read_csv( 'tags.csv' )\n",
    "bt = pd.read_csv( 'book_tags.csv')\n",
    "r = pd.read_csv( 'ratings.csv' )\n",
    "r = r.merge(b[['book_id','original_title']], on = 'book_id')\n",
    "bt = bt.merge( t, on = 'tag_id' )\n",
    "bt = bt.merge( b[[ 'goodreads_book_id', 'title','book_id']], on = 'goodreads_book_id' )\n",
    "\n",
    "r_high = r[r['rating']>4]\n",
    "r_high_by_user = r_high.sort_values('user_id')\n",
    "r_high_by_user = r_high_by_user[['user_id','book_id','original_title']]\n",
    "bt.dropna(inplace = True)\n",
    "r_high_by_user.dropna(inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744b6f2b",
   "metadata": {},
   "source": [
    "### sanity check... I was worried about the titles being different, but the different titles had differen book_ids\n",
    "Better safe than sorry!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c468ac03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52723 c length\n"
     ]
    }
   ],
   "source": [
    "book_id_to_correct_title = dict(zip(r.book_id,r.original_title))\n",
    "incorrect_title_to_book_id = dict(zip(bt.title, bt.book_id))\n",
    "\n",
    "book_id_to_correct_title_r = dict(zip(r.original_title, r.book_id))\n",
    "for key,value in book_id_to_correct_title_r.items():\n",
    "    if key not in incorrect_title_to_book_id.keys():\n",
    "        incorrect_title_to_book_id[key] = value\n",
    "\n",
    "bt.title = bt.title.apply(lambda x: book_id_to_correct_title[incorrect_title_to_book_id[x]]) #fix naming\n",
    "r.original_title = r.original_title.apply(lambda x: book_id_to_correct_title[incorrect_title_to_book_id[x]])\n",
    "r_high_by_user.original_title = r_high_by_user.original_title.apply(lambda x: book_id_to_correct_title[incorrect_title_to_book_id[x]])\n",
    "user_books_dict = gen_dict(r_high_by_user,'user_id','original_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6dc7e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.dropna(inplace=True)\n",
    "bt.dropna(inplace=True)\n",
    "\n",
    "book_list = list(set(list(r.original_title.unique())+list(bt.title.unique())))\n",
    "\n",
    "#create mapping from books to ints\n",
    "book_to_int = dict(zip(book_list,[i+1 for i in range(len(book_list))]))\n",
    "int_to_book = dict(zip([i+1 for i in range(len(book_list))],book_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda186a0",
   "metadata": {},
   "source": [
    "### Lots of the tags were uninformative/too much to turn into useful label, especially the ones with lots of instances like to-read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf369f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dict(collections.Counter(bt.tag_name))\n",
    "legal_tags = []\n",
    "for key, value in temp.items():\n",
    "    if value > 2 and value < 700:\n",
    "        legal_tags.append(key)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb486ef",
   "metadata": {},
   "source": [
    "### Generate dictionary for books with similar tags as well as useful dictionary for users and the titles they liked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "290578dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16332 c length\n",
      "53424 c length\n"
     ]
    }
   ],
   "source": [
    "for key, value in title_to_tags.items():\n",
    "    value_edit = list(filter((lambda x: x in legal_tags), value))\n",
    "    title_to_tags[key] = value_edit\n",
    "    \n",
    "tags_to_titles = {}\n",
    "temp = gen_dict(bt,'tag_name','title')\n",
    "for key,value in temp.items():\n",
    "    if key in legal_tags:\n",
    "        tags_to_titles[key] = value\n",
    "        \n",
    "        \n",
    "r_high_by_user.dropna(inplace=True)\n",
    "users_to_titles = gen_dict(r,'user_id','original_title')\n",
    "\n",
    "def GenerateContextDict(bookt_to_xs,x_to_books):\n",
    "    #returns dict where dict[title] = [b1,b2,...bn]\n",
    "    def flatten(t):\n",
    "        return [item for sublist in t for item in sublist]\n",
    "    \n",
    "    total_dict = {}\n",
    "    for key,value in bookt_to_xs.items():\n",
    "        counts = collections.Counter(flatten([(x_to_books[x]) for x in value]))\n",
    "        \n",
    "        \n",
    "        total_dict[key] = counts\n",
    "    \n",
    "        \n",
    "    return total_dict\n",
    "    \n",
    "    \n",
    "title_to_title_tags = GenerateContextDict(title_to_tags, tags_to_titles)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "582bfbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = {}\n",
    "for title, counter in title_to_title_tags.items():\n",
    "    temp[title] = {title_value:count for title_value, count in counter.items() if count >= 4}\n",
    "title_to_title_tags = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b35481",
   "metadata": {},
   "source": [
    "### Here is where we generate our positive pairs of words, i.e. words that should be embedded close together\n",
    " - We say books liked by the same user are similar and books which share a tag are similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "9beba67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneratePosSample(user, ex_per_user = 5, flatten = True):\n",
    "    #function for sampling 'similar' where we weight liklihood by frequency\n",
    "    books_user = random.choices(users_to_titles[user], k = ex_per_user)\n",
    "    books_tag = []\n",
    "    \n",
    "    bad_reads = 0 \n",
    "    for book in (books_user+books_user):\n",
    "        try:\n",
    "            tag_counter = title_to_title_tags[book]\n",
    "            tag = random.choices(list(tag_counter.keys()), k=1)[0]\n",
    "            if type(tag) == str:\n",
    "                books_tag.append((book_to_int[book],book_to_int[tag]))\n",
    "            else:\n",
    "                bad_reads +=1\n",
    "        except:\n",
    "            bad_reads += 1\n",
    "            \n",
    "    books_user += random.choices(users_to_titles[user], k=bad_reads)\n",
    "    \n",
    "    \n",
    "    book_ints_for_user_list = list(map(lambda book: book_to_int[book], books_user))\n",
    "    \n",
    "    shuffle = book_ints_for_user_list[:]\n",
    "    random.shuffle(shuffle)\n",
    "        \n",
    "    book_ints_for_user_list = list(zip(book_ints_for_user_list,shuffle))\n",
    "    \n",
    "    return book_ints_for_user_list + books_tag\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc7387",
   "metadata": {},
   "source": [
    "### Generate Training Data:\n",
    "Here is where we generate our full training data. For ever user, we generate num_ps positive pairse. For every positive pair, we generate num_ns negative pairs. Finally we bundle everything together and shuffle it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7e27488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VAL = max(book_to_int.values())+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "af4af197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(users_to_titles,num_ps, num_ns, vocab_size, book_to_int):\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    for i,user in enumerate(list(users_to_titles.keys())):\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "\n",
    "        positive_skip_grams = GeneratePosSample(user,num_ps)\n",
    "\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "\n",
    "            context_class = tf.expand_dims(\n",
    "              tf.constant([context_word], dtype=\"int32\"), 1)\n",
    "\n",
    "            negative_sampling_candidates = tf.random.uniform(shape=(num_ns,), minval=1, maxval=MAX_VAL, dtype=tf.int32)\n",
    "\n",
    "            negative_sampling_candidates = tf.expand_dims(\n",
    "              negative_sampling_candidates, 1)\n",
    "            \n",
    "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int32\")\n",
    "\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "            \n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "2d44ff6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "\n",
      "\n",
      "targets.shape: (2243808,)\n",
      "contexts.shape: (2243808, 15)\n",
      "labels.shape: (2243808, 15)\n"
     ]
    }
   ],
   "source": [
    "print('start')    \n",
    "targets, contexts, labels = generate_training_data(users_to_titles,14,14,10001, book_to_int)\n",
    "\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)[:,:,0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "7b5c33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364945c",
   "metadata": {},
   "source": [
    "### Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "dbf17777",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                          embedding_dim,\n",
    "                                          input_length=1,\n",
    "                                          name=\"w2v_embedding\")\n",
    "        self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                           embedding_dim,\n",
    "                                           input_length=14+1)\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "        return dots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5052a",
   "metadata": {},
   "source": [
    "### Finally its time to train! \n",
    "although accuracy and loss are shown, they are not super interpretable. For example a simple estimator f(b1,b2,$\\theta$) paramaterizing P(b1|b2) as f(b1,b2) = 1/num_ns will achieve high accruacy for a reasonable large number of negative examples. CategoricalCrossentropy is also opaque. All we care about is the final embeddings and that the metrixs are moving in the right directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "ba405b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "2243/2243 [==============================] - 25s 11ms/step - loss: 2.6036 - accuracy: 0.1595\n",
      "Epoch 2/7\n",
      "2243/2243 [==============================] - 32s 14ms/step - loss: 2.2825 - accuracy: 0.2600\n",
      "Epoch 3/7\n",
      "2243/2243 [==============================] - 30s 13ms/step - loss: 2.1680 - accuracy: 0.2864\n",
      "Epoch 4/7\n",
      "2243/2243 [==============================] - 24s 11ms/step - loss: 2.0793 - accuracy: 0.3108\n",
      "Epoch 5/7\n",
      "2243/2243 [==============================] - 23s 10ms/step - loss: 1.9995 - accuracy: 0.3339\n",
      "Epoch 6/7\n",
      "2243/2243 [==============================] - 23s 10ms/step - loss: 1.9304 - accuracy: 0.3555\n",
      "Epoch 7/7\n",
      "2243/2243 [==============================] - 24s 11ms/step - loss: 1.8725 - accuracy: 0.3744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1ae3677a10>"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model + run longer\n",
    "embedding_dim = 100\n",
    "vocab_size = len(book_to_int) + 1\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "word2vec.fit(dataset, epochs=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "f1962f41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save model embeddings\n",
    "#have to add line to file to get embed visualizer to work\n",
    "\n",
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "len(r.book_id.unique())\n",
    "\n",
    "for idx, book in list(int_to_book.items()):\n",
    "    if idx == 0:\n",
    "        pass\n",
    "    vec = weights[idx]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(book + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "e3a19d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = word2vec.target_embedding\n",
    "# model.save_weights('./model')\n",
    "\n",
    "with open(\"book_to_int.json\", \"w\") as outfile:\n",
    "    json.dump(book_to_int, outfile)\n",
    "    \n",
    "with open(\"int_to_book.json\", \"w\") as outfile:\n",
    "    json.dump(int_to_book, outfile)\n",
    "\n",
    "with open(\"book_id_to_correct_title.json\", \"w\") as outfile:\n",
    "    json.dump(book_id_to_correct_title, outfile)\n",
    "    \n",
    "with open(\"incorrect_title_to_book_id.json\", \"w\") as outfile:\n",
    "    json.dump(incorrect_title_to_book_id, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "6b4e6230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For using in actual recommendations\n",
    "most_popular_in_order = []\n",
    "books_by_popularity = pd.read_csv( 'books.csv' )\n",
    "books_by_popularity.dropna(inplace=True)\n",
    "\n",
    "for book_title in books_by_popularity.original_title.values:\n",
    "    if book_title in incorrect_title_to_book_id.keys():\n",
    "        book_id = incorrect_title_to_book_id[book_title]\n",
    "        correct_title = book_id_to_correct_title[book_id]\n",
    "        most_popular_in_order.append(correct_title)\n",
    "with open(\"most_popular_title_in_order\", \"w\") as outfile:\n",
    "    json.dump(most_popular_in_order, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "17779a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dict = {}\n",
    "for i in int_to_book:\n",
    "    embed_dict[i] = weights[i].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "40dc8fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"int_to_weight.json\", \"w\") as outfile:\n",
    "    json.dump(embed_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b511d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
